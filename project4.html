<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Frame Classifier</title>
    <style>
        ul {
            list-style-type: square; /* Change to disc, circle, square, or none */
        }
    </style>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <a href="index.html" class="return-button">
            <button>Return to Index</button>
        </a>
        <h1>Real-Time Game Frame Classifier</h1>
    </header>
    <section>
        <h2>Project Overview</h2>
        <p>For this project, I created and trained a convolutional neural network(cnn) using Torch with the goal of classifying the tone of video game screenshots. 
        Though intially intending to only deploy the model for inference on individual screenshots, I decided to add real time inference functionality
        so that predictions could be provided through an overlay over gameplay in real-time.</p>
        <p>Key libraries used:</p>
        <ul>
            <li><strong>Torch</strong>: Used to define, customize, and train the cnn.</li>
            <li><strong>CV2, MSS, PIL, Pygetwindow</strong>: Used during live inference for window capture and data handling.</li>
        </ul>
    </section>
    
    <section>
        <h2>Data Sources</h2>
        <p>The data used for training consists of around 900 png images with a source resolution of 3440x1440. 
          All images were captured by myself via splitting of gameplay recordings across 16 different video games.</p>
        <p>Images are placed into directories correlating to their depicted tone: "Horror", "Action", or "Scenic".</p>
        <p>The qualifications an image needs for each category:</p>
        <ul>
            <li><strong>Horror</strong>: Depicts environment that would inflict some kind of horror in me if I were present.</li>
            <li><strong>Action</strong>: Action occurring in environment, such as gunshots, explosions, physical striking, impacts, etc.</li>
            <li><strong>Scenic</strong>: No action occurring, environment depicted is not immediately threatening and is well lit.</li>
        </ul>
    </section>
    
    <section>
        <h2>Data Preparation</h2>
        <p>Torchvision's transforms package provides many transformations to be used on raw images. </p>
        <p>Transformations applied:</p>
        <ul>
            <li><strong>Resize((224,224))</strong>: Resizes the images from 3440x1440 to 224x224. These dimensions were derived from the Imagenet training dataset.</li>
            <li><strong>ToTensor()</strong>: Transforms image into a tensor. </li>
            <li><strong>Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</strong>: Ensures input falls within a certain range. Mean and std parameters derived from Imagenet training dataset.</li>
        </ul>
        <p>Parameters derived from <a href="https://pytorch.org/hub/pytorch_vision_resnet/">pytorch</a>.</p>
        <p>After transforming the dataset, I created train and test sets that were .8 and .2 of the full dataset respectively and defined dataloaders for both. Batch sizes are set to 64.</p>
    </section>
    
    <section>
        <h2>Model Definition</h2>
        <p>The model utilizes three convolutional layers and three fully connected layers. There are also pooling layers following each convolutional layer.
           It expects an input with a shape [batch_size, 3, 224, 224], where 3 represents the input channels: red, green, and blue in an RGB image, and each 224 represents the input image length and width.</p>
        <img src="images/frameclassifier_Initial model summary.png" alt="model summary">
        <p>The summary above illustrates the transformations applied to a sample input with dimensions [1, 3, 224, 224]</p>
        <p>Each convolutional layer will increase the number of outputted channels as it applies its filters and extracts features. 
           Pooling layers will reduce the dimensions of the image while maintaining key information. In this case, the filter size for the pooling layer is 2, so both the image length and width will be halved.
           Upon reaching the fully connected layers, the output is flattened as fc layers expect a one-dimensional input. The fc layers will make a prediction using this input and return a vector representing a score for each class.</p>
    </section>
    
    <section>
        <h2>Training</h2>
        <p>Currently, training for 5 epochs yields the best results. Cross Entropy Loss and accuracy in both the training and test sets were tracked throughout the process. Adam is used to optimize weights and zero the gradients before 
           each forward pass. </p>
        <p>Accuracy and Loss plots:</p>
        <img src="images/frameclassifier_Initial model acc_plot.png" alt="acc plot">
        <img src="images/frameclassifier_Initial model loss_plot.png" alt="loss plot">
    </section>
    
    <section>
        <h2>Live Inference</h2>
        <p>Using the libaries cv2, mss, PIL, and pygetwindow, the model can classify frames as they are given in real-time gameplay. The gameplay window is captured, and each frame is processed the same as the training dataset and classified.
           An overlay over the window depicts what the current frame is classified as. </p>
        <video width="640" height="360" controls>
        <source src="videos/sh3_rtd.mp4" type="video/mp4">
        </video>
        <p>This demo depicts the model classifying gameplay from "Silent Hill 3", a horror game. The model performs well and seems to classify the dimly lit, dingy environment correctly as having a horror tone.</p>
        <video width="640" height="360" controls>
        <source src="videos/dbz_rtd.mp4" type="video/mp4">
        </video>
        <p>This demo depicts the model classifying gameplay from "Dragon Ball Sparking! Zero, a fighting game. The model shows some shortcomings in this scenario; occasionally, there are some misclassified frames where action scenes
           are labeled scenic.</p>
        <video width="640" height="360" controls>
        <source src="videos/gmod_rtd.mp4" type="video/mp4">
        </video>
        <p>This demo depicts the model classifying gameplay from "Garry's Mod". The scene contains both dimily lit corridors and well-lit spaces. The model struggles with consistently identifying the bright environment as scenic.
           The brightness of the image may be misconstrued as indicative of action.</p>
        <p>The model's performance is satisfactory in horror-leaning environments, but leaves more to be desired when classifying between action and scenic.</p>
    </section>

    <section>
        <h2>Future Work</h2>
        <p></p>
    </section>
    
    <section>
        <h2>Code and Implementation</h2>
        <p>The source code for this project is available on <a href="https://github.com/Keegodude/Sentify">GitHub</a>. The following files are included:</p>
        <ul>
            <li><strong>app.py</strong>: The main Python script for data fetching and analysis.</li>
            <li><strong>init_db.py</strong>: Script for initializing SQLAlchemy/SQLite database.</li>
            <li><strong>config.py</strong>: Contains initialization data.</li>
            <li><strong>requirements.txt</strong>: List of required Python packages.</li>
        </ul>
    </section>
    
    <section>
        <h2>Project Takeaways</h2>
        <p>As with most projects working with a dataset; I reinforced my skills with handling/preparing data. Since this project's concept was devised from scratch,
        I had to brainstorm a good amount to come to a conclusion on what I wanted to do. I think this will benefit my ability to come up with plans to handle any prompts I may be given in the future.
        In using spotipy and lyricsgenius, I now have an understanding of interfacing with an api to retrieve data from those sources. I also reinforced my understanding of other concepts I've used in the past,
            such as principal component analysis, k-means clustering, and general visualization. </p>
    </section>
</body>
</html>
