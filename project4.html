<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Frame Classifier</title>
    <style>
        ul {
            list-style-type: square; /* Change to disc, circle, square, or none */
        }
    </style>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <a href="/" class="return-button">
            <button>Return to Index</button>
        </a>
        <h1>Real-Time Game Frame Classifier</h1>
    </header>

    <section>
        <h2>Project Overview</h2>
        <p>For this project, I created and trained a Convolutional Neural Network (CNN) using PyTorch to classify the tone of video game screenshots.
        Initially, the model was designed for individual image inference, but I later added real-time inference functionality, enabling predictions through an overlay on live gameplay.</p>
        <p>Key libraries used:</p>
        <ul>
            <li><strong>PyTorch</strong>: Used to define, customize, and train the CNN.</li>
            <li><strong>OpenCV (cv2), MSS, PIL, PyGetWindow</strong>: Used for real-time window capture and data handling.</li>
        </ul>
    </section>

    <section>
        <h2>Data Sources</h2>
        <p>The dataset consists of approximately 900 PNG images with a source resolution of 3440x1440, captured from gameplay recordings across 16 different video games.</p>
        <p>Images are categorized into three tone-based directories: "Horror," "Action," and "Scenic." The classification criteria are as follows:</p>
        <ul>
            <li><strong>Horror</strong>: Dark or eerie environments that evoke a sense of fear.</li>
            <li><strong>Action</strong>: Scenes with gunfire, explosions, physical combat, or other high-intensity events.</li>
            <li><strong>Scenic</strong>: Well-lit, non-threatening environments with no immediate action.</li>
        </ul>
    </section>

    <section>
        <h2>Data Preparation</h2>
        <p>Using Torchvision’s <code>transforms</code> package, I applied various transformations to improve model generalizability. Initially, flip and crop transformations were not included, but their addition improved performance on unseen games.</p>
        <p>Transformations applied:</p>
        <ul>
            <li><strong>Resize((224,224))</strong>: Resizes images from 3440x1440 to 224x224, based on ImageNet standards.</li>
            <li><strong>RandomHorizontalFlip(p=0.5)</strong>: Horizontally flips an image with a 50% probability.</li>
            <li><strong>RandomVerticalFlip(p=0.5)</strong>: Vertically flips an image with a 50% probability.</li>
            <li><strong>RandomResizedCrop(size=224, scale=(0.8, 1.0))</strong>: Crops 80% of the original resolution and resizes it.</li>
            <li><strong>ToTensor()</strong>: Converts images into tensors.</li>
            <li><strong>Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</strong>: Normalizes inputs based on ImageNet training statistics.</li>
        </ul>
        <p>After applying transformations, I split the dataset into training (80%) and test (20%) sets and defined DataLoaders with a batch size of 32.</p>
    </section>

    <section>
        <h2>Model Definition</h2>
        <p>The model consists of three convolutional layers, three fully connected layers, and pooling layers after each convolution. The expected input shape is <code>[batch_size, 3, 224, 224]</code>, where 3 represents the RGB channels.</p>
        <img src="images/frameclassifier_Initial model summary.png" alt="Model Summary">
        <p>The summary above illustrates transformations applied to a sample input.</p>
    </section>

    <section>
        <h2>Training</h2>
        <p>Training for five epochs produced optimal results. Cross-entropy loss and accuracy were tracked throughout the process, using Adam optimization to adjust weights and zero gradients.</p>
        <p>Accuracy and loss plots:</p>
        <img src="images/frameclassifier_Initial model acc_plot.png" alt="Accuracy Plot">
        <img src="images/frameclassifier_Initial model loss_plot.png" alt="Loss Plot">
    </section>

    <section>
        <h2>Live Inference</h2>
        <p>Utilizing OpenCV, MSS, PIL, and PyGetWindow, the model classifies frames in real time. The gameplay window is captured, processed, and classified, with an overlay displaying the results.</p>

        <video width="800" height="400" controls>
            <source src="videos/sh3_rtd.mp4" type="video/mp4">
        </video>
        <p>Silent Hill 3 – Correctly identifies horror tone.</p>

        <video width="800" height="400" controls>
            <source src="videos/dbz_rtd.mp4" type="video/mp4">
        </video>
        <p>Dragon Ball Sparking! Zero – Some action scenes misclassified as scenic.</p>

        <video width="800" height="400" controls>
            <source src="videos/gmod_rtd.mp4" type="video/mp4">
        </video>
        <p>Garry’s Mod – Inconsistent classification of bright environments.</p>
    </section>

    <section>
        <h2>Future Work</h2>
        <p>Next steps include tuning CNN parameters, experimenting with different pooling methods, and increasing dataset diversity. I also plan to explore deeper architectures leveraging my available hardware.</p>
    </section>

    <section>
        <h2>Code and Implementation</h2>
        <p>The source code for this project is available on <a href="https://github.com/Keegodude/vg-frame-classifier">GitHub</a>.</p>
    </section>

    <section>
        <h2>Project Takeaways</h2>
        <p>This project was my first hands-on experience with computer vision. Implementing my own CNN deepened my understanding of data preprocessing, model architecture, and real-time inference optimization.</p>
    </section>
</body>
</html>
